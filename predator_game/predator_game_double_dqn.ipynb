{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import ModelIntervalCheckpoint\n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "style.use('ggplot')\n",
    "import keras.backend as K\n",
    "\n",
    "dtype='float16'\n",
    "K.set_floatx(dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Cube:\n",
    "    def __init__(self, size):  # 随机初始化位置坐标\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, self.size - 1)\n",
    "        self.y = np.random.randint(0, self.size - 1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.x},{self.y}'\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x - other.x, self.y - other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choise):\n",
    "        if choise == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choise == 1:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choise == 2:\n",
    "            self.move(x=1, y=-1)\n",
    "        elif choise == 3:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choise == 4:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choise == 5:\n",
    "            self.move(x=0, y=-1)\n",
    "        elif choise == 6:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choise == 7:\n",
    "            self.move(x=-1, y=0)\n",
    "        elif choise == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        if self.x > self.size - 1:\n",
    "            self.x = self.size - 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        if self.y > self.size - 1:\n",
    "            self.y = self.size - 1\n",
    "\n",
    "\n",
    "class envCube:\n",
    "    SIZE = 10\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
    "    # OBSERVATION_SPACE_VALUES = (4,)\n",
    "    ACTION_SPACE_VALUES = 9\n",
    "    RETURN_IMAGE = True  # 考虑返回值是否图像\n",
    "\n",
    "    FOOD_REWARD = 25  # agent获得食物的奖励\n",
    "    ENEMY_PENALITY = -300  # 遇上对手的惩罚\n",
    "    MOVE_PENALITY = -1  # 每移动一步的惩罚\n",
    "\n",
    "    # 设定三个部分的颜色分别是蓝、绿、红\n",
    "    d = {1: (255, 0, 0),  # blue\n",
    "         2: (0, 255, 0),  # green\n",
    "         3: (0, 0, 255)}  # red\n",
    "    PLAYER_N = 1\n",
    "    FOOD_N = 2\n",
    "    ENEMY_N = 3\n",
    "\n",
    "    # 环境重置\n",
    "    def reset(self):\n",
    "        self.player = Cube(self.SIZE)\n",
    "        self.food = Cube(self.SIZE)\n",
    "        self.enemy = Cube(self.SIZE)\n",
    "        # 如果玩家和食物初始位置相同，重置食物的位置，直到位置不同\n",
    "        while self.player == self.food:\n",
    "            self.food = Cube(self.SIZE)\n",
    "        # 如果敌人和玩家或食物的初始位置相同，重置敌人的位置，直到位置不同\n",
    "        while self.player == self.enemy or self.food == self.enemy:\n",
    "            self.enemy = Cube(self.SIZE)\n",
    "        # 判断观测是图像和数字\n",
    "        if self.RETURN_IMAGE:\n",
    "            observation = np.array(self.get_image()) / 255\n",
    "        else:\n",
    "            observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "        self.enemy.move()\n",
    "        self.food.move()\n",
    "        # 判断观测是图像和数字\n",
    "        if self.RETURN_IMAGE:\n",
    "            new_observation = np.array(self.get_image()) / 255\n",
    "        else:\n",
    "            new_observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        # 奖励\n",
    "        if self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        elif self.player == self.enemy:\n",
    "            reward = self.ENEMY_PENALITY\n",
    "        else:\n",
    "            reward = self.MOVE_PENALITY\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if self.player == self.food or self.player == self.enemy or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((800, 800))\n",
    "        cv2.imshow('Predator', np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]\n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        return img\n",
    "\n",
    "    def get_qtable(self, q_table_name=None):\n",
    "        # 初始化Q表格\n",
    "        if q_table_name is None:  # 如果没有表格提供，就随机初始化一个Q表格\n",
    "            q_table = {}\n",
    "            for x1 in range(-self.SIZE + 1, self.SIZE):\n",
    "                for y1 in range(-self.SIZE + 1, self.SIZE):\n",
    "                    for x2 in range(-self.SIZE + 1, self.SIZE):\n",
    "                        for y2 in range(-self.SIZE + 1, self.SIZE):\n",
    "                            q_table[(x1, y1, x2, y2)] = [np.random.randint(-5, 0) for i in\n",
    "                                                         range(self.ACTION_SPACE_VALUES)]\n",
    "        else:  # 提供了，就使用提供的Q表格\n",
    "            with open(q_table_name, 'rb') as f:\n",
    "                q_table = pickle.load(f)\n",
    "        return q_table\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 8, 8, 32)       896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1, 8, 8, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 6, 6, 32)       9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1, 6, 6, 32)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                36896     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 297       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,393\n",
      "Trainable params: 48,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Output dense_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model cannot be compiled because it has no loss to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     16\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(envCube\u001B[38;5;241m.\u001B[39mOBSERVATION_SPACE_VALUES, envCube\u001B[38;5;241m.\u001B[39mACTION_SPACE_VALUES)\n\u001B[0;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39msummary()\n\u001B[1;32m---> 18\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[1;32mc:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 587\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    589\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py:442\u001B[0m, in \u001B[0;36mModel.compile\u001B[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001B[0m\n\u001B[0;32m    438\u001B[0m training_utils_v1\u001B[38;5;241m.\u001B[39mprepare_sample_weight_modes(\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_training_endpoints, sample_weight_mode)\n\u001B[0;32m    441\u001B[0m \u001B[38;5;66;03m# Creates the model loss and weighted metrics sub-graphs.\u001B[39;00m\n\u001B[1;32m--> 442\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compile_weights_loss_and_weighted_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;66;03m# Functions for train, test and predict will\u001B[39;00m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;66;03m# be compiled lazily when required.\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;66;03m# This saves time when the user is not using all functions.\u001B[39;00m\n\u001B[0;32m    447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:587\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 587\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    589\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py:1525\u001B[0m, in \u001B[0;36mModel._compile_weights_loss_and_weighted_metrics\u001B[1;34m(self, sample_weights)\u001B[0m\n\u001B[0;32m   1512\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_metrics(\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs,\n\u001B[0;32m   1514\u001B[0m     targets\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_targets,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1517\u001B[0m     masks\u001B[38;5;241m=\u001B[39mmasks,\n\u001B[0;32m   1518\u001B[0m     return_weighted_metrics\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1520\u001B[0m \u001B[38;5;66;03m# Compute total loss.\u001B[39;00m\n\u001B[0;32m   1521\u001B[0m \u001B[38;5;66;03m# Used to keep track of the total loss value (stateless).\u001B[39;00m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# eg., total_loss = loss_weight_1 * output_1_loss_fn(...) +\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m#                   loss_weight_2 * output_2_loss_fn(...) +\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;66;03m#                   layer losses.\u001B[39;00m\n\u001B[1;32m-> 1525\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_total_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py:1620\u001B[0m, in \u001B[0;36mModel._prepare_total_loss\u001B[1;34m(self, masks)\u001B[0m\n\u001B[0;32m   1618\u001B[0m   loss_list\u001B[38;5;241m.\u001B[39mappend(loss_weight \u001B[38;5;241m*\u001B[39m output_loss)\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m loss_list \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlosses:\n\u001B[1;32m-> 1620\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe model cannot be compiled \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1621\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbecause it has no loss to optimize.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1623\u001B[0m \u001B[38;5;66;03m# Add regularization penalties and other layer-specific losses.\u001B[39;00m\n\u001B[0;32m   1624\u001B[0m custom_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_losses_for(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_losses_for(\n\u001B[0;32m   1625\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs)\n",
      "\u001B[1;31mValueError\u001B[0m: The model cannot be compiled because it has no loss to optimize."
     ]
    }
   ],
   "source": [
    "def build_model(status, nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(1,) + status))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(envCube.OBSERVATION_SPACE_VALUES, envCube.ACTION_SPACE_VALUES)\n",
    "model.summary()\n",
    "print(model.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_agent(model, nb_actions):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "                   enable_double_dqn=True, target_model_update=5000, policy=policy)\n",
    "    dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "    return dqn\n",
    "\n",
    "\n",
    "def build_duel_agent(model, nb_actions):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                                  nb_steps=500000)\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "                   enable_dueling_network=True, target_model_update=1e-2, dueling_type='avg', policy=policy)\n",
    "\n",
    "    dqn.compile(Adam(learning_rate=1e-3))\n",
    "    return dqn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = envCube()\n",
    "dqn = build_duel_agent(model, envCube.ACTION_SPACE_VALUES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_weights_filename = 'dqn_weights_{step}.h5f'\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=10000)]\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('duel_dqn_weights.h5f', overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dqn.load_weights('duel_dqn_weights.h5f')\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "score = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "print(np.mean(score.history['episode_reward']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}