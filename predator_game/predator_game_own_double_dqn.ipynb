{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from matplotlib import style\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.callbacks import ModelIntervalCheckpoint\n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "style.use('ggplot')\n",
    "from keras import mixed_precision\n",
    "# mixed_precision.set_global_policy('mixed_float16')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class Cube:\n",
    "    def __init__(self, size):  # 随机初始化位置坐标\n",
    "        self.size = size\n",
    "        self.x = np.random.randint(0, self.size - 1)\n",
    "        self.y = np.random.randint(0, self.size - 1)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.x},{self.y}'\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return (self.x - other.x, self.y - other.y)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choise):\n",
    "        if choise == 0:\n",
    "            self.move(x=1, y=1)\n",
    "        elif choise == 1:\n",
    "            self.move(x=-1, y=1)\n",
    "        elif choise == 2:\n",
    "            self.move(x=1, y=-1)\n",
    "        elif choise == 3:\n",
    "            self.move(x=-1, y=-1)\n",
    "        elif choise == 4:\n",
    "            self.move(x=0, y=1)\n",
    "        elif choise == 5:\n",
    "            self.move(x=0, y=-1)\n",
    "        elif choise == 6:\n",
    "            self.move(x=1, y=0)\n",
    "        elif choise == 7:\n",
    "            self.move(x=-1, y=0)\n",
    "        elif choise == 8:\n",
    "            self.move(x=0, y=0)\n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        if self.x > self.size - 1:\n",
    "            self.x = self.size - 1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        if self.y > self.size - 1:\n",
    "            self.y = self.size - 1\n",
    "\n",
    "\n",
    "class envCube:\n",
    "    SIZE = 10\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
    "    # OBSERVATION_SPACE_VALUES = (4,)\n",
    "    ACTION_SPACE_VALUES = 9\n",
    "    RETURN_IMAGE = True  # 考虑返回值是否图像\n",
    "\n",
    "    FOOD_REWARD = 25  # agent获得食物的奖励\n",
    "    ENEMY_PENALITY = -300  # 遇上对手的惩罚\n",
    "    MOVE_PENALITY = -1  # 每移动一步的惩罚\n",
    "\n",
    "    # 设定三个部分的颜色分别是蓝、绿、红\n",
    "    d = {1: (255, 0, 0),  # blue\n",
    "         2: (0, 255, 0),  # green\n",
    "         3: (0, 0, 255)}  # red\n",
    "    PLAYER_N = 1\n",
    "    FOOD_N = 2\n",
    "    ENEMY_N = 3\n",
    "\n",
    "    # 环境重置\n",
    "    def reset(self):\n",
    "        self.player = Cube(self.SIZE)\n",
    "        self.food = Cube(self.SIZE)\n",
    "        self.enemy = Cube(self.SIZE)\n",
    "        # 如果玩家和食物初始位置相同，重置食物的位置，直到位置不同\n",
    "        while self.player == self.food:\n",
    "            self.food = Cube(self.SIZE)\n",
    "        # 如果敌人和玩家或食物的初始位置相同，重置敌人的位置，直到位置不同\n",
    "        while self.player == self.enemy or self.food == self.enemy:\n",
    "            self.enemy = Cube(self.SIZE)\n",
    "        # 判断观测是图像和数字\n",
    "        if self.RETURN_IMAGE:\n",
    "            observation = np.array(self.get_image()) / 255\n",
    "        else:\n",
    "            observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        self.episode_step = 0\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.player.action(action)\n",
    "        self.enemy.move()\n",
    "        self.food.move()\n",
    "        # 判断观测是图像和数字\n",
    "        if self.RETURN_IMAGE:\n",
    "            new_observation = np.array(self.get_image()) / 255\n",
    "        else:\n",
    "            new_observation = (self.player - self.food) + (self.player - self.enemy)\n",
    "\n",
    "        # 奖励\n",
    "        if self.player == self.food:\n",
    "            reward = self.FOOD_REWARD\n",
    "        elif self.player == self.enemy:\n",
    "            reward = self.ENEMY_PENALITY\n",
    "        else:\n",
    "            reward = self.MOVE_PENALITY\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if self.player == self.food or self.player == self.enemy or self.episode_step >= 200:\n",
    "            done = True\n",
    "\n",
    "        return new_observation, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((800, 800))\n",
    "        cv2.imshow('Predator', np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def get_image(self):\n",
    "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)\n",
    "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]\n",
    "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]\n",
    "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]\n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        return img\n",
    "\n",
    "    def get_qtable(self, q_table_name=None):\n",
    "        # 初始化Q表格\n",
    "        if q_table_name is None:  # 如果没有表格提供，就随机初始化一个Q表格\n",
    "            q_table = {}\n",
    "            for x1 in range(-self.SIZE + 1, self.SIZE):\n",
    "                for y1 in range(-self.SIZE + 1, self.SIZE):\n",
    "                    for x2 in range(-self.SIZE + 1, self.SIZE):\n",
    "                        for y2 in range(-self.SIZE + 1, self.SIZE):\n",
    "                            q_table[(x1, y1, x2, y2)] = [np.random.randint(-5, 0) for i in\n",
    "                                                         range(self.ACTION_SPACE_VALUES)]\n",
    "        else:  # 提供了，就使用提供的Q表格\n",
    "            with open(q_table_name, 'rb') as f:\n",
    "                q_table = pickle.load(f)\n",
    "        return q_table\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self._train_dir = os.path.join(self._log_write_dir,'train')\n",
    "        self.step += 1\n",
    "        # self._val_dir = os.path.join(self._log_write_dir,'validation')\n",
    "        # self._val_step = 1\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        with self.writer.as_default():\n",
    "            for name, value in logs.items():\n",
    "                tf.summary.scalar(name, value, step=self.step)\n",
    "                self.writer.flush()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "env = envCube()\n",
    "\n",
    "class DQNAgent():\n",
    "    REPLAY_MEMORY_SIZE = 100\n",
    "    MINI_REPLAY_MEMORY_SIZE = 32\n",
    "    DISCOUNT = 0.95\n",
    "    UPDATE_TARGET_MODEL_SIZE_EVERY = 5\n",
    "    def __init__(self):\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.replay_memory = deque(maxlen=100)\n",
    "        self.update_target_model_count = 0\n",
    "        # self.tensorboard = ModifiedTensorBoard(log_dir=f'dqn_model_{int(time.time())}')\n",
    "        self.tensorboard = TensorBoard()\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32,(3,3),activation='relu',input_shape=envCube.OBSERVATION_SPACE_VALUES))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(envCube.ACTION_SPACE_VALUES, activation='linear'))\n",
    "        model.compile(loss='mse',optimizer='Adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self,terminal_state):\n",
    "        if len(self.replay_memory) < self.REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        minibatch = random.sample(self.replay_memory,self.REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        obs_batch = np.array([transition[0] for transition in minibatch])/255\n",
    "        new_obs_batch = np.array([transition[3] for transition in minibatch])/255\n",
    "        X = obs_batch\n",
    "        q_values_current = self.model.predict(obs_batch)\n",
    "        q_values_future = self.target_model.predict(new_obs_batch)\n",
    "\n",
    "        for index,(obs,action,reward,new_obs,done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                yt = reward + self.DISCOUNT*np.max(q_values_future[index])\n",
    "            else:\n",
    "                yt = reward\n",
    "            q_values_current_index = q_values_current[index]\n",
    "            q_values_current_index[action] = yt\n",
    "            y.append(q_values_current_index)\n",
    "\n",
    "        self.model.fit(np.array(X),np.array(y),batch_size=self.MINI_REPLAY_MEMORY_SIZE,shuffle=False,verbose=0,callbacks=[self.tensorboard]if terminal_state else None)\n",
    "        if terminal_state:\n",
    "            self.update_target_model_count += 1\n",
    "        if self.update_target_model_count > self.UPDATE_TARGET_MODEL_SIZE_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.update_target_model_count = 0\n",
    "\n",
    "    def update_replay_memory(self,transition):\n",
    "        return self.replay_memory.append(transition)\n",
    "\n",
    "    def action_q_values_predict(self,obs):\n",
    "        return self.model.predict(np.array(obs).reshape(-1,*obs.shape))[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]c:\\users\\zhou\\pycharmprojects\\reinforcement_learning\\venv\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "  6%|▌         | 166/3000 [06:07<1:44:33,  2.21s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "EPISODES = 3000    # 局数\n",
    "epsilon = 1\n",
    "EPS_DECAY = 0.995\n",
    "SHOW_EVERY = 30   # 定义每隔多少局展示一次图像\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(EPISODES)):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(agent.action_q_values_predict(obs))   # 选择Q值最高的动作，来进行开发\n",
    "        else:\n",
    "            action = np.random.randint(0,env.ACTION_SPACE_VALUES)\n",
    "        new_obs, reward, done,_ = env.step(action)\n",
    "        transition = (obs,action,reward,new_obs,done)\n",
    "        agent.update_replay_memory(transition)\n",
    "\n",
    "        agent.train(done)\n",
    "        obs = new_obs\n",
    "        episode_reward += reward\n",
    "\n",
    "    # print('episode ',episode,'episode_reward:',episode_reward)\n",
    "\n",
    "    epsilon *= EPS_DECAY\n",
    "    epsilon = max(epsilon,0.001)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    # if episode % SHOW_EVERY == 0:\n",
    "    #     agent.tensorboard.update_stats(avg_reward=np.mean(episode_rewards[-SHOW_EVERY:]),max_reward=np.max(episode_rewards[-SHOW_EVERY:]),min_reward=np.min(episode_rewards[-SHOW_EVERY:]),epsilon=epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SHOW_EVERY = 300\n",
    "moving_avg = np.convolve(episode_rewards, np.ones((SHOW_EVERY,))/SHOW_EVERY,mode='valid')\n",
    "print(len(moving_avg))\n",
    "plt.plot([i for i in range(len(moving_avg))], moving_avg)\n",
    "plt.xlabel('episode #')\n",
    "plt.ylabel(f'mean{SHOW_EVERY} reward')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}